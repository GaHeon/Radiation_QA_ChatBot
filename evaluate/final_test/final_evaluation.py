import concurrent.futures
import csv
from datetime import datetime
import json
import os
import re
import sys
import threading
import time
import hashlib
from dotenv import load_dotenv

from anthropic import AnthropicVertex
from google.api_core import exceptions
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from tqdm import tqdm
import numpy as np
import pandas as pd
from vertexai.preview.generative_models import GenerativeModel, Part
import vertexai

# --- Configuration ---
MODEL_LIST = ["gemini-2.0-flash"]
EVALUATOR_MODEL = "gemini-2.5-pro"
HYDE_MODEL = "gemini-2.0-flash"
COMPRESSION_MODEL = "gemini-2.0-flash"

QUESTIONS_FILE = "eval_question_30.jsonl"
RESULTS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "result")
HYDE_CACHE_FILE = "hyde_cache.json"

PROJECT_ID = "turing-berm-q3bf2"
LOCATION = "us-east5"

# --- ì´ˆê¸°í™” ---    
db = None
clients = {}
hyde_cache = {}

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)
load_dotenv(os.path.join(project_root, '.env'))

def initialize_vertexai():
    """Vertex AIë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global PROJECT_ID
    print("Vertex AI ì´ˆê¸°í™” ì‹œì‘...")
    PROJECT_ID = os.getenv("PROJECT_ID")
    if not PROJECT_ID:
        print("ì˜¤ë¥˜: .env íŒŒì¼ì— PROJECT_IDë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        sys.exit(1)
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    print(f"Vertex AI ì´ˆê¸°í™” ì™„ë£Œ. (Project: {PROJECT_ID}, Location: {LOCATION})")
    return True

def load_vector_db():
    """FAISS ë²¡í„° DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    global db
    if db: return True
    print("FAISS ë²¡í„° DB ë¡œë”© ì‹œì‘...")
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/distiluse-base-multilingual-cased-v1")
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        local_path = os.path.join(project_root, "embed_faiss")
        if not os.path.exists(local_path):
            print(f"ì˜¤ë¥˜: FAISS ë²¡í„° DB ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {local_path}")
            return False
        db = FAISS.load_local(local_path, embeddings, allow_dangerous_deserialization=True)
        print("ë²¡í„° DB ë¡œë”© ì™„ë£Œ.")
        return True
    except Exception as e:
        print(f"ë²¡í„° DB ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def load_hyde_cache():
    """HyDE ìºì‹œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    global hyde_cache
    if os.path.exists(HYDE_CACHE_FILE):
        try:
            with open(HYDE_CACHE_FILE, 'r', encoding='utf-8') as f:
                hyde_cache = json.load(f)
            print(f"HyDE ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(hyde_cache)}ê°œ í•­ëª©")
        except Exception as e:
            print(f"HyDE ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            hyde_cache = {}
    else:
        hyde_cache = {}

def save_hyde_cache():
    """HyDE ìºì‹œë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with open(HYDE_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(hyde_cache, f, ensure_ascii=False, indent=2)
        print(f"HyDE ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(hyde_cache)}ê°œ í•­ëª©")
    except Exception as e:
        print(f"HyDE ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_client(model_name: str):
    """Vertex AI ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    global clients
    if model_name not in clients:
        print(f"INFO: '{model_name}' ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ë¡œë”© ì¤‘...")
        if model_name.startswith("claude"):
            if 'anthropic_vertex' not in clients:
                clients['anthropic_vertex'] = AnthropicVertex(project_id=PROJECT_ID, region=LOCATION)
            clients[model_name] = clients['anthropic_vertex']
        else:
            clients[model_name] = GenerativeModel(model_name)
    return clients[model_name]

def count_tokens_approximate(text: str) -> int:
    """í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    # í•œêµ­ì–´ì™€ ì˜ì–´ í˜¼ì¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê·¼ì‚¬ì¹˜ ê³„ì‚°
    # ì˜ì–´: í‰ê·  4ê¸€ìë‹¹ 1í† í°, í•œêµ­ì–´: í‰ê·  1.5ê¸€ìë‹¹ 1í† í°
    korean_chars = len([c for c in text if ord(c) >= 0xAC00 and ord(c) <= 0xD7A3])
    other_chars = len(text) - korean_chars
    return int(korean_chars / 1.5 + other_chars / 4)

def generate_with_retry_and_token_tracking(client, model_name: str, prompt: str, max_retries: int = 3):
    """í† í° ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•˜ë©´ì„œ ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    for attempt in range(max_retries):
        try:
            response_text = ""
            input_tokens = count_tokens_approximate(prompt)  # ì…ë ¥ í† í° ê·¼ì‚¬ì¹˜
            output_tokens = 0
            
            if model_name.startswith("claude"):
                response = client.messages.create(
                    model=model_name, 
                    max_tokens=2048, 
                    messages=[{"role": "user", "content": prompt}]
                )
                response_text = response.content[0].text
                # Claude APIì—ì„œ í† í° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                if hasattr(response, 'usage'):
                    input_tokens = response.usage.input_tokens
                    output_tokens = response.usage.output_tokens
                else:
                    output_tokens = count_tokens_approximate(response_text)
            else: # Gemini
                response = client.generate_content(prompt)
                if response.candidates and response.candidates[0].content.parts:
                    response_text = response.candidates[0].content.parts[0].text
                    output_tokens = count_tokens_approximate(response_text)
                
                # Geminiì˜ í† í° ì •ë³´ëŠ” ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê·¼ì‚¬ì¹˜ ì‚¬ìš©
                if hasattr(response, 'usage_metadata'):
                    input_tokens = getattr(response.usage_metadata, 'prompt_token_count', input_tokens)
                    output_tokens = getattr(response.usage_metadata, 'candidates_token_count', output_tokens)
            
            if response_text.strip():
                return {
                    'text': response_text.strip(),
                    'input_tokens': input_tokens,
                    'output_tokens': output_tokens,
                    'total_tokens': input_tokens + output_tokens
                }
            else:
                print(f"  - ê²½ê³ : ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. (ì‹œë„ {attempt + 1}/{max_retries})")

        except exceptions.ServiceUnavailable as e:
            wait_time = 2 ** (attempt + 1)
            print(f"  - ê²½ê³ : ì„œë¹„ìŠ¤ ì¼ì‹œ ì¤‘ë‹¨(503). {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì‹œë„ {attempt + 1}/{max_retries})")
            time.sleep(wait_time)
        except Exception as e:
            print(f"  - ì˜¤ë¥˜: ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            break
    
    print(f"  - ì˜¤ë¥˜: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries}) í›„ì—ë„ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨.")
    return {
        'text': f"ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (ëª¨ë¸: {model_name})",
        'input_tokens': input_tokens,
        'output_tokens': 0,
        'total_tokens': input_tokens
    }

def generate_hyde_answer(question: str):
    """HyDE (Hypothetical Document Embeddings)ë¥¼ ìœ„í•´ ê°€ìƒì˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    question_hash = hashlib.sha256(question.encode('utf-8')).hexdigest()
    if question_hash in hyde_cache:
        print(f"     - (HyDE) ìºì‹œì—ì„œ ì¡°íšŒ: '{question[:30]}...'")
        return {'text': hyde_cache[question_hash], 'total_tokens': 0}
    
    client = get_client(HYDE_MODEL)
    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì´ìƒì ì¸ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ì´ ë‹µë³€ì€ ì‚¬ì‹¤ì´ ì•„ë‹ˆì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤. 
ì˜¤ì§ ë²¡í„° ê²€ìƒ‰ì˜ í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•œ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ë‹µë³€ì€ ìƒì„¸í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€:"""
    
    result = generate_with_retry_and_token_tracking(client, HYDE_MODEL, prompt)
    if result['text'] and "ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨" not in result['text']:
        hyde_cache[question_hash] = result['text']
        print(f"     - (HyDE) ìƒˆë¡œ ìƒì„±: '{question[:30]}...' -> '{result['text'][:50]}...'")
        return result
    else:
        print(f"     - (HyDE) ìƒì„± ì‹¤íŒ¨: '{question[:30]}...'. ì›ë³¸ ì§ˆë¬¸ì„ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return {'text': question, 'total_tokens': 0}

def compress_documents(question: str, documents: list):
    """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì¥ë§Œ ì¶”ì¶œí•˜ì—¬ ë¬¸ì„œë¥¼ ì••ì¶•í•©ë‹ˆë‹¤."""
    if not documents:
        return {'text': "", 'total_tokens': 0}
    
    client = get_client(COMPRESSION_MODEL)
    
    all_content = "\n\n".join([f"ë¬¸ì„œ {i+1}: {doc.page_content}" for i, doc in enumerate(documents)])
    
    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ê´€ë ¨ ë¬¸ì¥ë“¤ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì œì™¸í•˜ê³ , í•µì‹¬ ì •ë³´ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ì°¸ê³  ë¬¸ì„œ:
{all_content}

ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ì •ë¦¬:"""
    
    result = generate_with_retry_and_token_tracking(client, COMPRESSION_MODEL, prompt)
    if result['text'] and "ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨" not in result['text']:
        return result
    else:
        return {'text': all_content, 'total_tokens': 0}

def evaluate_response(question, answer, context, prompt_type, token_info):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ì„ í‰ê°€í•˜ê³  ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    evaluator_client = get_client(EVALUATOR_MODEL)
    
    if prompt_type == "RAG (Document-based)":
        prompt = f"""ë‹¹ì‹ ì€ ë‹µë³€ì˜ í’ˆì§ˆì„ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ëŠ” í‰ê°€ìì…ë‹ˆë‹¤. ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ 0ì ì—ì„œ 5ì  ì‚¬ì´ë¡œ ì±„ì í•˜ê³ , í‰ê°€ ê·¼ê±°ë¥¼ í•œêµ­ì–´ë¡œ ê°„ëµí•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.

[í‰ê°€ ê¸°ì¤€]
1. ì •í™•ì„± (Accuracy): [ì±—ë´‡ ì‘ë‹µ]ì´ ì‚¬ì‹¤ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€.
2. ì¶©ì‹¤ë„ (Faithfulness): [ì±—ë´‡ ì‘ë‹µ]ì´ **ì˜¤ì§ [ì°¸ê³  ë¬¸ì„œ]ì— ëª…ì‹œëœ ì •ë³´ë§Œì„ ì‚¬ìš©**í•˜ì—¬ ìƒì„±ë˜ì—ˆëŠ”ì§€. **ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©, ì™¸ë¶€ ì§€ì‹, ë˜ëŠ” ê³¼ì¥ëœ í•´ì„ì´ í¬í•¨ë˜ì—ˆë‹¤ë©´ 0ì ì„ ë¶€ì—¬í•˜ì„¸ìš”.**
3. ê´€ë ¨ì„± (Relevance): [ì±—ë´‡ ì‘ë‹µ]ì´ [ì§ˆë¬¸]ì˜ ì˜ë„ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€.
4. ì „ë¬¸ì„± (Domain Appropriateness): ë°©ì‚¬ì„  QA ë„ë©”ì¸ì— ì ì ˆí•œ í‘œí˜„ê³¼ ì§€ì‹ì„ ì‚¬ìš©í–ˆëŠ”ì§€.
5. ì™„ê²°ì„± (Completeness): ë‹µë³€ì´ ì§ˆë¬¸ì— ëŒ€í•´ 'ì°¸ê³  ë¬¸ì„œ' ë‚´ì˜ í•µì‹¬ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ê³  ìˆëŠ”ì§€.

[í‰ê°€í•  ë°ì´í„°]
- ì‚¬ìš©ì ì§ˆë¬¸: "{question}"
- ì°¸ê³  ë¬¸ì„œ: "{context}"
- í‰ê°€í•  ë‹µë³€: "{answer}"

[ì¶œë ¥ í˜•ì‹]
```json
{{"ì •í™•ì„±": {{"score": [0-5], "reason": "í‰ê°€ ê·¼ê±°"}}, "ì¶©ì‹¤ë„": {{"score": [0-5], "reason": "í‰ê°€ ê·¼ê±°"}}, "ê´€ë ¨ì„±": {{"score": [0-5], "reason": "í‰ê°€ ê·¼ê±°"}}, "ì „ë¬¸ì„±": {{"score": [0-5], "reason": "í‰ê°€ ê·¼ê±°"}}, "ì™„ê²°ì„±": {{"score": [0-5], "reason": "í‰ê°€ ê·¼ê±°"}}}}
```"""
    else:  # Fallback
        prompt = f"""ë‹¹ì‹ ì€ ë‹µë³€ì˜ í’ˆì§ˆì„ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ëŠ” í‰ê°€ìì…ë‹ˆë‹¤. ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ 0ì ì—ì„œ 5ì  ì‚¬ì´ë¡œ ì±„ì í•˜ê³ , í‰ê°€ ê·¼ê±°ë¥¼ í•œêµ­ì–´ë¡œ ê°„ëµí•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.

[í‰ê°€ ê¸°ì¤€]
1. ì •í™•ì„± (Accuracy): [ì±—ë´‡ ì‘ë‹µ]ì´ ì‚¬ì‹¤ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€.
2. ì¶©ì‹¤ë„ (Faithfulness): ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ë‹µë³€ì´ë¯€ë¡œ ì´ í•­ëª©ì€ 5ì ìœ¼ë¡œ ê³ ì •í•©ë‹ˆë‹¤.
3. ê´€ë ¨ì„± (Relevance): [ì±—ë´‡ ì‘ë‹µ]ì´ [ì§ˆë¬¸]ì˜ ì˜ë„ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€.
4. ì „ë¬¸ì„± (Domain Appropriateness): ë°©ì‚¬ì„  QA ë„ë©”ì¸ì— ì ì ˆí•œ í‘œí˜„ê³¼ ì§€ì‹ì„ ì‚¬ìš©í–ˆëŠ”ì§€.
5. ì™„ê²°ì„± (Completeness): ë‹µë³€ì´ ì§ˆë¬¸ì˜ ëª¨ë“  ì¸¡ë©´ì„ ì¶©ë¶„íˆ ë‹¤ë£¨ê³  ìˆëŠ”ì§€.

[í‰ê°€í•  ë°ì´í„°]
- ì‚¬ìš©ì ì§ˆë¬¸: "{question}"
- í‰ê°€í•  ë‹µë³€: "{answer}"

[ì¶œë ¥ í˜•ì‹]
```json
{{"ì •í™•ì„±": {{"score": [0-5], "reason": "í‰ê°€ ê·¼ê±°"}}, "ì¶©ì‹¤ë„": {{"score": 5, "reason": "ì¼ë°˜ ì§€ì‹ ë‹µë³€"}}, "ê´€ë ¨ì„±": {{"score": [0-5], "reason": "í‰ê°€ ê·¼ê±°"}}, "ì „ë¬¸ì„±": {{"score": [0-5], "reason": "í‰ê°€ ê·¼ê±°"}}, "ì™„ê²°ì„±": {{"score": [0-5], "reason": "í‰ê°€ ê·¼ê±°"}}}}
```"""

    evaluation_result = generate_with_retry_and_token_tracking(evaluator_client, EVALUATOR_MODEL, prompt)
    
    scores = extract_scores(evaluation_result['text'])
    
    scores['token_info'] = {
        'hyde_tokens': token_info.get('hyde_tokens', 0),
        'condense_tokens': token_info.get('condense_tokens', 0),
        'compression_tokens': token_info.get('compression_tokens', 0),
        'answer_tokens': token_info.get('answer_tokens', 0),
        'evaluation_tokens': evaluation_result['total_tokens'],
        'total_tokens': sum([
            token_info.get('hyde_tokens', 0),
            token_info.get('condense_tokens', 0),
            token_info.get('compression_tokens', 0),
            token_info.get('answer_tokens', 0),
            evaluation_result['total_tokens']
        ])
    }
    
    return scores

def extract_scores(text):
    """ì‘ë‹µì—ì„œ JSON ì ìˆ˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    text = re.sub(r'```json\s*|\s*```', '', text)
    
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if not json_match:
        print(f"Error: ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì‘ë‹µ ë‚´ìš©: {text}")
        return {"ì •í™•ì„±": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}, "ì¶©ì‹¤ë„": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}, "ê´€ë ¨ì„±": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}, "ì „ë¬¸ì„±": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}, "ì™„ê²°ì„±": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}}

    json_str = json_match.group(0)
    try:
        result = json.loads(json_str)
        return {
            "ì •í™•ì„±": result.get("ì •í™•ì„±", {"score": 0, "reason": "ì ìˆ˜ ì—†ìŒ"}),
            "ì¶©ì‹¤ë„": result.get("ì¶©ì‹¤ë„", {"score": 0, "reason": "ì ìˆ˜ ì—†ìŒ"}),
            "ê´€ë ¨ì„±": result.get("ê´€ë ¨ì„±", {"score": 0, "reason": "ì ìˆ˜ ì—†ìŒ"}),
            "ì „ë¬¸ì„±": result.get("ì „ë¬¸ì„±", {"score": 0, "reason": "ì ìˆ˜ ì—†ìŒ"}),
            "ì™„ê²°ì„±": result.get("ì™„ê²°ì„±", {"score": 0, "reason": "ì ìˆ˜ ì—†ìŒ"})
        }
    except json.JSONDecodeError as e:
        print(f"Error: JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\níŒŒì‹± ëŒ€ìƒ: {json_str}\nì—ëŸ¬: {e}")
        return {"ì •í™•ì„±": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}, "ì¶©ì‹¤ë„": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}, "ê´€ë ¨ì„±": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}, "ì „ë¬¸ì„±": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}, "ì™„ê²°ì„±": {"score": 0, "reason": "JSON íŒŒì‹± ì‹¤íŒ¨"}}

def save_results_to_csv(test_run_id, question, model_name, prompt_type, answer, scores, token_info):
    """ê²°ê³¼ë¥¼ CSV íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    os.makedirs(RESULTS_DIR, exist_ok=True)
    filename = f"final_evaluation_results_{test_run_id}.csv"
    filepath = os.path.join(RESULTS_DIR, filename)
    
    file_exists = os.path.exists(filepath)
    
    with open(filepath, 'a', newline='', encoding='utf-8') as csvfile:
        fieldnames = [
            'TestRunID', 'Question', 'Model', 'PromptType', 'Answer',
            'Accuracy_Score', 'Accuracy_Reason', 'Faithfulness_Score', 'Faithfulness_Reason',
            'Relevance_Score', 'Relevance_Reason', 'Domain_Score', 'Domain_Reason',
            'Completeness_Score', 'Completeness_Reason',
            'HyDE_Tokens', 'Condense_Tokens', 'Compression_Tokens', 'Answer_Tokens', 
            'Evaluation_Tokens', 'Total_Tokens', 'Timestamp'
        ]
        
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        if not file_exists:
            writer.writeheader()
        
        writer.writerow({
            'TestRunID': test_run_id,
            'Question': question,
            'Model': model_name,
            'PromptType': prompt_type,
            'Answer': answer,
            'Accuracy_Score': scores.get('ì •í™•ì„±', {}).get('score', 0),
            'Accuracy_Reason': scores.get('ì •í™•ì„±', {}).get('reason', ''),
            'Faithfulness_Score': scores.get('ì¶©ì‹¤ë„', {}).get('score', 0),
            'Faithfulness_Reason': scores.get('ì¶©ì‹¤ë„', {}).get('reason', ''),
            'Relevance_Score': scores.get('ê´€ë ¨ì„±', {}).get('score', 0),
            'Relevance_Reason': scores.get('ê´€ë ¨ì„±', {}).get('reason', ''),
            'Domain_Score': scores.get('ì „ë¬¸ì„±', {}).get('score', 0),
            'Domain_Reason': scores.get('ì „ë¬¸ì„±', {}).get('reason', ''),
            'Completeness_Score': scores.get('ì™„ê²°ì„±', {}).get('score', 0),
            'Completeness_Reason': scores.get('ì™„ê²°ì„±', {}).get('reason', ''),
            'HyDE_Tokens': token_info.get('hyde_tokens', 0),
            'Condense_Tokens': token_info.get('condense_tokens', 0),
            'Compression_Tokens': token_info.get('compression_tokens', 0),
            'Answer_Tokens': token_info.get('answer_tokens', 0),
            'Evaluation_Tokens': token_info.get('evaluation_tokens', 0),
            'Total_Tokens': token_info.get('total_tokens', 0),
            'Timestamp': datetime.now().isoformat()
        })
    
    return filepath

def load_questions_from_jsonl(file_path, num_questions=30):
    """JSONL íŒŒì¼ì—ì„œ ì§ˆë¬¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    questions = []
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    if not os.path.isabs(file_path):
        file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), file_path)
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line_num > num_questions:
                    break
                line = line.strip()
                if line:
                    try:
                        data = json.loads(line)
                        questions.append(data)
                    except json.JSONDecodeError as e:
                        print(f"Warning: JSON íŒŒì‹± ì‹¤íŒ¨ (ë¼ì¸ {line_num}): {e}")
        print(f"ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ: {len(questions)}ê°œ")
        return questions
    except FileNotFoundError:
        print(f"Error: ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return []

def print_summary(results):
    """ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    if not results:
        print("ì¶œë ¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    model_stats = {}
    total_tokens = 0
    
    for result in results:
        model = result['model']
        if model not in model_stats:
            model_stats[model] = {
                'count': 0,
                'accuracy': [],
                'faithfulness': [],
                'relevance': [],
                'domain': [],
                'completeness': [],
                'total_tokens': 0
            }
        
        model_stats[model]['count'] += 1
        model_stats[model]['accuracy'].append(result['scores']['ì •í™•ì„±']['score'])
        model_stats[model]['faithfulness'].append(result['scores']['ì¶©ì‹¤ë„']['score'])
        model_stats[model]['relevance'].append(result['scores']['ê´€ë ¨ì„±']['score'])
        model_stats[model]['domain'].append(result['scores']['ì „ë¬¸ì„±']['score'])
        model_stats[model]['completeness'].append(result['scores']['ì™„ê²°ì„±']['score'])
        model_stats[model]['total_tokens'] += result['token_info']['total_tokens']
        total_tokens += result['token_info']['total_tokens']
    
    for model, stats in model_stats.items():
        print(f"\n--- ëª¨ë¸: {model} (ì´ {stats['count']}ê°œ ì§ˆë¬¸) ---")
        print(f"  - í‰ê·  ì •í™•ì„±: {np.mean(stats['accuracy']):.2f} / 5")
        print(f"  - í‰ê·  ì¶©ì‹¤ë„: {np.mean(stats['faithfulness']):.2f} / 5")
        print(f"  - í‰ê·  ê´€ë ¨ì„±: {np.mean(stats['relevance']):.2f} / 5")
        print(f"  - í‰ê·  ì „ë¬¸ì„±: {np.mean(stats['domain']):.2f} / 5")
        print(f"  - í‰ê·  ì™„ê²°ì„±: {np.mean(stats['completeness']):.2f} / 5")
        print(f"  - ì „ì²´ í‰ê·  ì ìˆ˜: {np.mean([np.mean(stats['accuracy']), np.mean(stats['faithfulness']), np.mean(stats['relevance']), np.mean(stats['domain']), np.mean(stats['completeness'])]):.2f} / 5")
        print(f"  - ì´ í† í° ì‚¬ìš©ëŸ‰: {stats['total_tokens']:,} í† í°")
        print(f"  - ì§ˆë¬¸ë‹¹ í‰ê·  í† í°: {stats['total_tokens'] // stats['count']:,} í† í°")
    
    print(f"\n--- ì „ì²´ í†µê³„ ---")
    print(f"  - ì´ í‰ê°€ ì§ˆë¬¸ ìˆ˜: {len(results)}")
    print(f"  - ì´ í† í° ì‚¬ìš©ëŸ‰: {total_tokens:,} í† í°")
    print(f"  - ì§ˆë¬¸ë‹¹ í‰ê·  í† í°: {total_tokens // len(results):,} í† í°")
    
    prompt_types = {}
    for result in results:
        prompt_type = result['prompt_type']
        if prompt_type not in prompt_types:
            prompt_types[prompt_type] = 0
        prompt_types[prompt_type] += 1
    
    print(f"\n--- í”„ë¡¬í”„íŠ¸ íƒ€ì…ë³„ ì‚¬ìš© í˜„í™© ---")
    for prompt_type, count in prompt_types.items():
        print(f"  - {prompt_type}: {count}ê°œ ì§ˆë¬¸")

def prepare_data_for_evaluation(question: str):
    """ì§ˆë¬¸ì— ëŒ€í•œ RAG ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."""
    print(f"\nğŸ” ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: '{question[:50]}...'")
    
    token_info = {
        'hyde_tokens': 0,
        'condense_tokens': 0,
        'compression_tokens': 0,
        'answer_tokens': 0,
        'evaluation_tokens': 0,
        'total_tokens': 0
    }
    
    # 1. HyDE ìƒì„±
    print("     - HyDE ìƒì„± ì¤‘...")
    hyde_result = generate_hyde_answer(question)
    token_info['hyde_tokens'] = hyde_result.get('total_tokens', 0)
    hyde_answer = hyde_result['text']
    
    # 2. ì§ˆë¬¸ ìš”ì•½
    print("     - ì§ˆë¬¸ ìš”ì•½ ì¤‘...")
    condensed_result = generate_with_retry_and_token_tracking(
        get_client(HYDE_MODEL), 
        HYDE_MODEL, 
        f"ì§ˆë¬¸ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”: {question}"
    )
    token_info['condense_tokens'] = condensed_result['total_tokens']
    condensed_question = condensed_result['text']
    
    # 3. ë²¡í„° ê²€ìƒ‰ (HyDE ê¸°ë°˜)
    print("     - ë²¡í„° ê²€ìƒ‰ ì¤‘...")
    docs = db.similarity_search_with_score(hyde_answer, k=10)
    
    # 4. ìœ ì‚¬ë„ ê¸°ë°˜ RAG/Fallback ê²°ì •
    if docs and docs[0][1] <= 1.0:  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì¡°ì • (0.5 -> 1.0)
        prompt_type = "RAG (Document-based)"
        print("     - RAG ëª¨ë“œë¡œ ë‹µë³€ ìƒì„±...")
        
        # 5. ë¬¸ì„œ ì••ì¶•
        print("     - ë¬¸ì„œ ì••ì¶• ì¤‘...")
        compression_result = compress_documents(question, [doc for doc, score in docs if score < 1.0]) # ì„ê³„ê°’ ì¡°ì •
        token_info['compression_tokens'] = compression_result['total_tokens']
        
        context = compression_result['text']
    else:
        prompt_type = "Fallback (General Knowledge)"
        print("     - Fallback ëª¨ë“œë¡œ ë‹µë³€ ìƒì„±...")
        context = "ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ë‹µë³€"
    
    return {
        'question': question,
        'context': context,
        'prompt_type': prompt_type,
        'token_info': token_info
    }

def generate_single_answer_task(args):
    """ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± íƒœìŠ¤í¬"""
    question, model_name = args
    
    # ë°ì´í„° ì¤€ë¹„
    data = prepare_data_for_evaluation(question)
    context = data['context']
    prompt_type = data['prompt_type']
    token_info = data['token_info']
    
    # ë‹µë³€ ìƒì„±
    client = get_client(model_name)
    
    if prompt_type == "RAG (Document-based)":
        prompt = f"""ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ì‚¬ì„  ì¥ë¹„ í’ˆì§ˆê´€ë¦¬(QA) ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.

[ì°¸ê³  ë¬¸ì„œ]ëŠ” ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ìœ ì¼í•œ ì •ë³´ ì†ŒìŠ¤ì…ë‹ˆë‹¤. **ì ˆëŒ€ ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ë‹¹ì‹ ì˜ ê¸°ì¡´ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**

[ì°¸ê³  ë¬¸ì„œ]:
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]:
{question}

[ì‘ë‹µ ì§€ì¹¨]:
1.  **ë‹µë³€ ìƒì„±**: [ì‚¬ìš©ì ì§ˆë¬¸]ì— ë‹µí•˜ê¸° ìœ„í•´, [ì°¸ê³  ë¬¸ì„œ]ì˜ ê´€ë ¨ ë‚´ìš©ì„ ì¢…í•©í•˜ê³  ìš”ì•½í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì „ë¬¸ê°€ì˜ ì„¤ëª…ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
2.  **ì •ë³´ ë¶€ì¡± ì‹œ**: ë§Œì•½ [ì°¸ê³  ë¬¸ì„œ]ì˜ ë‚´ìš©ë§Œìœ¼ë¡œëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ê¸°ì— ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´, ë‹µë³€ì„ ì§€ì–´ë‚´ì§€ ë§ê³  "ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ê³  ì„œë‘ë¥¼ ì‹œì‘í•˜ë©° ë¬¸ì„œì—ì„œ ì°¾ì€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë§Œìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
3.  **ì™„ì „í•œ ì •ë³´ ë¶€ì¬ ì‹œ**: ë§Œì•½ [ì°¸ê³  ë¬¸ì„œ]ì— ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ì „í˜€ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
4.  **ìŠ¤íƒ€ì¼**: ë‹µë³€ì€ ëª…í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ì „ë¬¸ê°€ì˜ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”.

[ë‹µë³€]:"""
    else:
        prompt = f"""ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ì‘ë‹µ ì§€ì¹¨: ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ ì‹œì‘ ì‹œ, "ì œê°€ ê°€ì§„ ë¬¸ì„œì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì§€ë§Œ, ì¼ë°˜ì ì¸ ì§€ì‹ì— ë”°ë¥´ë©´" ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.

ë‹µë³€:"""
    
    answer_result = generate_with_retry_and_token_tracking(client, model_name, prompt)
    token_info['answer_tokens'] = answer_result['total_tokens']
    
    return {
        'question': question,
        'model': model_name,
        'answer': answer_result['text'],
        'context': context,
        'prompt_type': prompt_type,
        'token_info': token_info
    }

def evaluate_question_across_models(args):
    """í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª¨ë“  ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” íƒœìŠ¤í¬"""
    question, model_name = args
    
    # ë‹µë³€ ìƒì„±
    answer_data = generate_single_answer_task((question, model_name))
    
    # ë‹µë³€ í‰ê°€
    scores_with_tokens = evaluate_response(
        answer_data['question'],
        answer_data['answer'],
        answer_data['context'],
        answer_data['prompt_type'],
        answer_data['token_info']
    )
    
    # ìµœì¢… í† í° ì •ë³´ë¥¼ scores_with_tokensì—ì„œ ë¶„ë¦¬
    final_token_info = scores_with_tokens.pop('token_info')
    
    return {
        'question': answer_data['question'],
        'model': answer_data['model'],
        'answer': answer_data['answer'],
        'context': answer_data['context'],
        'prompt_type': answer_data['prompt_type'],
        'scores': scores_with_tokens,
        'token_info': final_token_info
    }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ìµœì¢… ì±—ë´‡ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*80)
    
    # ì´ˆê¸°í™”
    if not initialize_vertexai():
        return
    if not load_vector_db():
        return
    
    # HyDE ìºì‹œ ë¡œë“œ
    load_hyde_cache()
    
    # ì§ˆë¬¸ ë¡œë“œ
    questions = load_questions_from_jsonl(QUESTIONS_FILE)
    if not questions:
        print("ì§ˆë¬¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ID ìƒì„±
    test_run_id = f"FinalTest_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    print(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ID: {test_run_id}")
    
    # ëª¨ë“  ì§ˆë¬¸-ëª¨ë¸ ì¡°í•© ìƒì„±
    tasks = [(item['question'], model) for item in questions for model in MODEL_LIST]
    print(f"ì´ {len(tasks)}ê°œì˜ í‰ê°€ íƒœìŠ¤í¬ ì¤€ë¹„ ì™„ë£Œ")
    
    # ë³‘ë ¬ ì‹¤í–‰
    results = []
    csv_filepath = None
    
    print(f"\nğŸ”„ ë³‘ë ¬ í‰ê°€ ì‹œì‘...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        future_to_task = {executor.submit(evaluate_question_across_models, task): task for task in tasks}
        
        for future in tqdm(concurrent.futures.as_completed(future_to_task), total=len(tasks), desc="í‰ê°€ ì§„í–‰ë¥ "):
            try:
                result = future.result()
                results.append(result)
                
                # CSVì— ì¦‰ì‹œ ì €ì¥
                csv_filepath = save_results_to_csv(
                    test_run_id,
                    result['question'],
                    result['model'],
                    result['prompt_type'],
                    result['answer'],
                    result['scores'],
                    result['token_info']
                )
                
                print(f"\nâœ… ì™„ë£Œ: {result['model']} - '{result['question'][:30]}...'")
                print(f"   - í”„ë¡¬í”„íŠ¸ íƒ€ì…: {result['prompt_type']}")
                print(f"   - í‰ê·  ì ìˆ˜: {np.mean([result['scores']['ì •í™•ì„±']['score'], result['scores']['ì¶©ì‹¤ë„']['score'], result['scores']['ê´€ë ¨ì„±']['score'], result['scores']['ì „ë¬¸ì„±']['score'], result['scores']['ì™„ê²°ì„±']['score']]):.2f}/5")
                print(f"   - í† í° ì‚¬ìš©ëŸ‰: {result['token_info']['total_tokens']:,}")
                
            except Exception as e:
                task = future_to_task[future]
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {task} - {e}")
    
    # HyDE ìºì‹œ ì €ì¥
    save_hyde_cache()
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print_summary(results)
    
    print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {csv_filepath}")
    print("ğŸ‰ ìµœì¢… í‰ê°€ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 