import os
import json
import time
import numpy as np
from vertexai.generative_models import GenerativeModel
from google.api_core import exceptions
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from tqdm import tqdm
import concurrent.futures

# Configuration
# í‰ê°€ ì§ˆë¬¸ íŒŒì¼ ê²½ë¡œ
EVAL_QUESTIONS_FILE = 'eval_questions_hardware_qa_30.jsonl'
# í‰ê°€ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
EVAL_RESULTS_FILE = 'evaluate_results_hardware_qa.json'

# --- 1. ëª¨ë¸ ë° DB ë¡œë“œ ---
print("ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° DB ë¡œë”© ì‹œì‘...")
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/distiluse-base-multilingual-cased-v1"
)
# ìŠ¤í¬ë¦½íŠ¸ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì•„ embed_faiss í´ë” ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(script_dir, '..'))
local_path = os.path.join(project_root, "embed_faiss")

if not os.path.exists(local_path):
    raise FileNotFoundError(f"Embeddings directory not found at: {local_path}. Please ensure 'embed_faiss' directory is in the project root.")

db = FAISS.load_local(local_path, embeddings, allow_dangerous_deserialization=True)
print("ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° DB ë¡œë”© ì™„ë£Œ.")

def generate_with_retry(model, prompt: str, max_retries: int = 3):
    """
    ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë‚˜ ë¹ˆ ì‘ë‹µ ë°œìƒ ì‹œ ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•˜ì—¬ ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    retries = 0
    while retries < max_retries:
        try:
            response = model.generate_content(prompt, stream=False)
            # .textì— ì ‘ê·¼ ì‹œ ValueErrorê°€ ë°œìƒí•˜ë©´ ì‘ë‹µì´ ë¹„ì—ˆê±°ë‚˜ ì°¨ë‹¨ëœ ê²ƒì…ë‹ˆë‹¤.
            # ì´ ì ‘ê·¼ ìì²´ê°€ ìœ íš¨ì„± ê²€ì‚¬ ì—­í• ì„ í•©ë‹ˆë‹¤.
            return response.text.strip()
        except exceptions.ServiceUnavailable as e:
            retries += 1
            wait_time = 2 ** retries  # 2, 4, 8ì´ˆ í›„ ì¬ì‹œë„
            print(f"\nWarning: Service Unavailable. {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì‹œë„ {retries}/{max_retries})")
            time.sleep(wait_time)
        except ValueError:
            # ì½˜í…ì¸ ê°€ ì•ˆì „ ì„¤ì • ë“±ì— ì˜í•´ í•„í„°ë§ë˜ì–´ .text ì ‘ê·¼ ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°
            print(f"\nWarning: ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤ (Safety/Filtering). í•´ë‹¹ í•­ëª©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return ""
        except Exception as e:
            # ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ (e.g. AttributeError)
            print(f"\nì½˜í…ì¸  ìƒì„± ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
    print(f"\nError: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    return ""

def load_eval_questions():
    """ì§ˆë¬¸ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # ì„¤ì •ì— ì§€ì •ëœ ì§ˆë¬¸ íŒŒì¼ì„ ì‚¬ìš©í•˜ë„ë¡ ê²½ë¡œ ìˆ˜ì •
        file_path = os.path.join(script_dir, EVAL_QUESTIONS_FILE)
        with open(file_path, "r", encoding="utf-8") as f:
            return [json.loads(line) for line in f]
    except FileNotFoundError:
        print(f"Error: ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {EVAL_QUESTIONS_FILE}")
        return []

def get_similarity_score(answer, reference):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([answer, reference])
    sim_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
    return float(sim_matrix[0][0])

def get_judge_evaluation(question, answer, context):
    model = GenerativeModel("gemini-2.0-flash")
    prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸, ì±—ë´‡ì˜ ì‘ë‹µ, ê·¸ë¦¬ê³  ì‘ë‹µ ìƒì„±ì˜ ê·¼ê±°ê°€ ëœ ì°¸ê³  ë¬¸ì„œì…ë‹ˆë‹¤.

[ì§ˆë¬¸]:
{question}

[ì°¸ê³  ë¬¸ì„œ]:
{context}

[ì±—ë´‡ ì‘ë‹µ]:
{answer}

ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ 0~5ì ìœ¼ë¡œ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”:

1. ì •í™•ì„± (Accuracy): [ì±—ë´‡ ì‘ë‹µ]ì´ ì‚¬ì‹¤ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€.
2. ì¶©ì‹¤ë„ (Faithfulness): [ì±—ë´‡ ì‘ë‹µ]ì´ **ì˜¤ì§ [ì°¸ê³  ë¬¸ì„œ]ì— ëª…ì‹œëœ ì •ë³´ë§Œì„ ì‚¬ìš©**í•˜ì—¬ ìƒì„±ë˜ì—ˆëŠ”ì§€. **ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©, ì™¸ë¶€ ì§€ì‹, ë˜ëŠ” ê³¼ì¥ëœ í•´ì„ì´ í¬í•¨ë˜ì—ˆë‹¤ë©´ 0ì ì„ ë¶€ì—¬í•˜ì„¸ìš”.**
3. ê´€ë ¨ì„± (Relevance): [ì±—ë´‡ ì‘ë‹µ]ì´ [ì§ˆë¬¸]ì˜ ì˜ë„ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€.
4. ì „ë¬¸ì„± (Domain Appropriateness): ë°©ì‚¬ì„  QA ë„ë©”ì¸ì— ì ì ˆí•œ í‘œí˜„ê³¼ ì§€ì‹ì„ ì‚¬ìš©í–ˆëŠ”ì§€.
5. í‘œí˜„ë ¥ (Fluency): ë¬¸ì¥ì´ ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•˜ë©° ë§¤ë„ëŸ½ê²Œ ì „ë‹¬ë˜ëŠ”ì§€.

ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "ì •í™•ì„±": 0~5,
  "ì¶©ì‹¤ë„": 0~5,
  "ê´€ë ¨ì„±": 0~5,
  "ì „ë¬¸ì„±": 0~5,
  "í‘œí˜„ë ¥": 0~5
}}
"""
    judge_text = generate_with_retry(model, prompt)
    return judge_text

def translate_query_to_english(model, query: str):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ í•œê¸€ ì¿¼ë¦¬ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤."""
    prompt = f"Translate the following Korean text to English. Do not add any extra explanation. Just provide the translated text.\n\nKorean: {query}\n\nEnglish:"
    
    translated_text = generate_with_retry(model, prompt)
    
    # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
    if not translated_text:
        print(f"\nWarning: Query translation failed for '{query}'. Using original query.")
        return query
    
    return translated_text

def generate_hypothetical_answer(model, question: str):
    """
    HyDE (Hypothetical Document Embeddings)ë¥¼ ìœ„í•´ ê°€ìƒì˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì´ìƒì ì¸ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ì´ ë‹µë³€ì€ ì‚¬ì‹¤ì´ ì•„ë‹ˆì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤. 
ì˜¤ì§ ë²¡í„° ê²€ìƒ‰ì˜ í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•œ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ë‹µë³€ì€ ìƒì„¸í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€:
"""
    hypothetical_answer = generate_with_retry(model, prompt)
    if not hypothetical_answer:
        print(f"\nWarning: ê°€ìƒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: '{question}'. ì›ë³¸ ì§ˆë¬¸ì„ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return question
    return hypothetical_answer

def extract_scores(text):
    text = re.sub(r'```json\s*|\s*```', '', text)
    
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if not json_match:
        print(f"Error: ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì‘ë‹µ ë‚´ìš©: {text}")
        return {"ì •í™•ì„±": 0, "ì¶©ì‹¤ë„": 0, "ê´€ë ¨ì„±": 0, "ì „ë¬¸ì„±": 0, "í‘œí˜„ë ¥": 0}

    json_str = json_match.group(0)
    try:
        result = json.loads(json_str)
        return {
            "ì •í™•ì„±": result.get("ì •í™•ì„±", 0),
            "ì¶©ì‹¤ë„": result.get("ì¶©ì‹¤ë„", 0),
            "ê´€ë ¨ì„±": result.get("ê´€ë ¨ì„±", 0),
            "ì „ë¬¸ì„±": result.get("ì „ë¬¸ì„±", 0),
            "í‘œí˜„ë ¥": result.get("í‘œí˜„ë ¥", 0)
        }
    except json.JSONDecodeError as e:
        print(f"Error: JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\níŒŒì‹± ëŒ€ìƒ: {json_str}\nì—ëŸ¬: {e}")
        return {"ì •í™•ì„±": 0, "ì¶©ì‹¤ë„": 0, "ê´€ë ¨ì„±": 0, "ì „ë¬¸ì„±": 0, "í‘œí˜„ë ¥": 0}

def process_question(item):
    """
    í•˜ë‚˜ì˜ ì§ˆë¬¸ í•­ëª©ì— ëŒ€í•œ ì „ì²´ RAG í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    (ë¬¸ì„œ ê²€ìƒ‰ -> ë‹µë³€ ìƒì„± -> ë‹µë³€ í‰ê°€)
    """
    # ê° ìŠ¤ë ˆë“œì—ì„œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ìƒì„±í•©ë‹ˆë‹¤.
    model = GenerativeModel("gemini-2.0-flash")
    
    question = item["question"]
    reference = item.get("answer", "")

    # 1. HyDE: ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ ê°€ìƒì˜ ë‹µë³€ ìƒì„±
    hypothetical_answer = generate_hypothetical_answer(model, question)
    if hypothetical_answer != question:
        tqdm.write(f"[HyDE] ê°€ìƒ ë‹µë³€ ìƒì„± ì™„ë£Œ for '{question}'")

    # 2. ê°€ìƒ ë‹µë³€ì„ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰ (k=10ìœ¼ë¡œ ëŠ˜ë ¤ì„œ ë” ë§ì€ context í™•ë³´)
    docs = db.similarity_search(hypothetical_answer, k=10)
    context = "\n\n".join(doc.page_content for doc in docs)

    # 3. ë‹µë³€ ìƒì„± ì‹œì—ëŠ” ì›ë³¸ í•œê¸€ ì§ˆë¬¸ ì‚¬ìš©
    user_prompt = f"""ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ì‚¬ì„  ì¥ë¹„ í’ˆì§ˆê´€ë¦¬(QA) ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.

[ì°¸ê³  ë¬¸ì„œ]ëŠ” ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ìœ ì¼í•œ ì •ë³´ ì†ŒìŠ¤ì…ë‹ˆë‹¤. **ì ˆëŒ€ ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ë‹¹ì‹ ì˜ ê¸°ì¡´ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**

[ì°¸ê³  ë¬¸ì„œ]:
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]:
{question}

[ì‘ë‹µ ì§€ì¹¨]:
1.  **ë‹µë³€ ìƒì„±**: [ì‚¬ìš©ì ì§ˆë¬¸]ì— ë‹µí•˜ê¸° ìœ„í•´, [ì°¸ê³  ë¬¸ì„œ]ì˜ ê´€ë ¨ ë‚´ìš©ì„ ì¢…í•©í•˜ê³  ìš”ì•½í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì „ë¬¸ê°€ì˜ ì„¤ëª…ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
2.  **ì •ë³´ ë¶€ì¡± ì‹œ**: ë§Œì•½ [ì°¸ê³  ë¬¸ì„œ]ì˜ ë‚´ìš©ë§Œìœ¼ë¡œëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ê¸°ì— ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´, ë‹µë³€ì„ ì§€ì–´ë‚´ì§€ ë§ê³  "ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ê³  ì„œë‘ë¥¼ ì‹œì‘í•˜ë©° ë¬¸ì„œì—ì„œ ì°¾ì€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë§Œìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
3.  **ì™„ì „í•œ ì •ë³´ ë¶€ì¬ ì‹œ**: ë§Œì•½ [ì°¸ê³  ë¬¸ì„œ]ì— ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ì „í˜€ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
4.  **ìŠ¤íƒ€ì¼**: ë‹µë³€ì€ ëª…í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ì „ë¬¸ê°€ì˜ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”.

[ë‹µë³€]:
"""
    response_text = generate_with_retry(model, user_prompt)
    if not response_text:
        print(f"\nì§ˆë¬¸({question})ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨. í•´ë‹¹ í•­ëª©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None # ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

    judge_result = get_judge_evaluation(question, response_text, context)
    if not judge_result:
        print(f"\nì§ˆë¬¸({question})ì— ëŒ€í•œ í‰ê°€ ì‹¤íŒ¨. í•´ë‹¹ í•­ëª©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        score_dict = {"ì •í™•ì„±": 0, "ì¶©ì‹¤ë„": 0, "ê´€ë ¨ì„±": 0, "ì „ë¬¸ì„±": 0, "í‘œí˜„ë ¥": 0}
    else:
        score_dict = extract_scores(judge_result)
    
    sim_score = get_similarity_score(response_text, reference) if reference else None

    return {
        "question": question,
        "response": response_text,
        "scores": score_dict,
        "similarity": round(sim_score, 4) if sim_score else "N/A"
    }

def evaluate():
    questions = load_eval_questions()
    if not questions:
        print("í‰ê°€í•  ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    results = []
    
    # ëª¨ë“  ì ìˆ˜ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
    all_scores = {
        "ì •í™•ì„±": [], "ì¶©ì‹¤ë„": [], "ê´€ë ¨ì„±": [], "ì „ë¬¸ì„±": [], "í‘œí˜„ë ¥": []
    }
    all_similarities = []
    not_found_count = 0

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ë¡œ ì§ˆë¬¸ ì²˜ë¦¬ (ìµœëŒ€ 10ê°œ ë™ì‹œ ì‘ì—…)
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        # ì‘ì—…ì„ ì œì¶œí•˜ê³  future ê°ì²´ë¥¼ ë°›ìŒ
        future_to_question = {executor.submit(process_question, item): item for item in questions}
        
        # tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë¥  í‘œì‹œ
        for future in tqdm(concurrent.futures.as_completed(future_to_question), total=len(questions), desc="Evaluating RAG pipeline"):
            result = future.result()
            if result:
                results.append(result)

    # ëª¨ë“  ê²°ê³¼ê°€ ìˆ˜ì§‘ëœ í›„ ì ìˆ˜ ì§‘ê³„
    for res in results:
        score_dict = res['scores']
        for key in all_scores:
            all_scores[key].append(score_dict.get(key, 0))
        
        if res['similarity'] != "N/A":
            all_similarities.append(res['similarity'])
        
        # "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ" ì‘ë‹µ ì¹´ìš´íŠ¸
        if "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in res.get("response", ""):
            not_found_count += 1

    # ê²°ê³¼ íŒŒì¼ì„ evaluate í´ë” ë‚´ì— ì €ì¥í•˜ë„ë¡ ê²½ë¡œ ìˆ˜ì •
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(script_dir, "evaluate_results_answerable.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ í‰ê°€ ì™„ë£Œ: {output_path}")

    # --- ìµœì¢… í‰ê°€ ê²°ê³¼ (í‰ê·  ì ìˆ˜) ---
    print("\n--- ìµœì¢… í‰ê°€ ê²°ê³¼ (í‰ê·  ì ìˆ˜) ---")
    avg_scores = {}
    for key, scores in all_scores.items():
        avg_scores[key] = np.mean(scores) if scores else 0
        print(f"- {key}: {avg_scores[key]:.2f} / 5")

    if all_similarities:
        avg_similarity = np.mean(all_similarities)
        print(f"- ìœ ì‚¬ë„ (Similarity): {avg_similarity:.4f}")

    if questions:
        not_found_rate = (not_found_count / len(questions)) * 100
        print(f"ğŸ“Š ê²€ìƒ‰ ì‹¤íŒ¨ìœ¨: {not_found_rate:.2f}% ({not_found_count}/{len(questions)})")
    
    print("---------------------------------")

    # í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # ì„¤ì •ì— ì§€ì •ëœ ê²°ê³¼ íŒŒì¼ì„ ì‚¬ìš©í•˜ë„ë¡ ê²½ë¡œ ìˆ˜ì •
    results_path = os.path.join(script_dir, EVAL_RESULTS_FILE)
    
    final_summary_with_results = {
        "summary": {
            "ì •í™•ì„±": avg_scores["ì •í™•ì„±"],
            "ì¶©ì‹¤ë„": avg_scores["ì¶©ì‹¤ë„"],
            "ê´€ë ¨ì„±": avg_scores["ê´€ë ¨ì„±"],
            "ì „ë¬¸ì„±": avg_scores["ì „ë¬¸ì„±"],
            "í‘œí˜„ë ¥": avg_scores["í‘œí˜„ë ¥"]
        },
        "individual_results": results
    }

    with open(results_path, "w", encoding="utf-8") as f:
        json.dump(final_summary_with_results, f, ensure_ascii=False, indent=4)
    
    print(f"\ní‰ê°€ ê²°ê³¼ê°€ {results_path} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    evaluate()
